{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchinfo\n# !pip install torchmetrics\n# !pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:17.485287Z","iopub.execute_input":"2023-06-16T09:00:17.485711Z","iopub.status.idle":"2023-06-16T09:00:28.892277Z","shell.execute_reply.started":"2023-06-16T09:00:17.485680Z","shell.execute_reply":"2023-06-16T09:00:28.891044Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/gdrive')\n# from google.colab.patches import cv2_imshow as imshow","metadata":{"id":"MOTB8XYEW8ga","colab":{"base_uri":"https://localhost:8080/"},"outputId":"068eb634-14b1-40ce-9619-eb98075c8d8f","execution":{"iopub.status.busy":"2023-06-16T09:00:28.895182Z","iopub.execute_input":"2023-06-16T09:00:28.895595Z","iopub.status.idle":"2023-06-16T09:00:28.901616Z","shell.execute_reply.started":"2023-06-16T09:00:28.895555Z","shell.execute_reply":"2023-06-16T09:00:28.900500Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# !unzip \"/content/gdrive/MyDrive/Dataset.zip\" -d \"/content/gdrive/MyDrive/SAR_Dataset\"","metadata":{"id":"v9tKY8gHXPl0","execution":{"iopub.status.busy":"2023-06-16T09:00:28.903406Z","iopub.execute_input":"2023-06-16T09:00:28.903744Z","iopub.status.idle":"2023-06-16T09:00:28.915419Z","shell.execute_reply.started":"2023-06-16T09:00:28.903713Z","shell.execute_reply":"2023-06-16T09:00:28.914429Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport numpy as np\nimport os\nimport torchinfo\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"id":"ny4v2BrVX5Tx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ebadbdb4-2410-46e2-e364-ed5b1c6d9c60","execution":{"iopub.status.busy":"2023-06-16T09:00:28.919740Z","iopub.execute_input":"2023-06-16T09:00:28.920071Z","iopub.status.idle":"2023-06-16T09:00:28.927332Z","shell.execute_reply.started":"2023-06-16T09:00:28.920046Z","shell.execute_reply":"2023-06-16T09:00:28.926302Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class SARData(Dataset):\n    def __init__(self):\n        \n        self.xtrain_path = '/kaggle/input/sar8bit/Dataset/img_dir' #Path to inputs\n        self.ytrain_path = '/kaggle/input/sar8bit/Dataset/ann_dir_8bit'#Path to labels\n\n        self.xtrain_imgs = np.array(os.listdir(self.xtrain_path))[:5000]\n        self.ytrain_imgs = np.array(os.listdir(self.ytrain_path))[:5000]\n        self.num_classes = 5\n        self.height, self.width = 256,256\n\n    def process_label(self, label):\n        r = []\n        for i in range(self.num_classes):\n            mask = label == i+1\n            mask = mask.float()\n            r.append(mask)\n        return torch.stack(r)\n    \n    def __len__(self):\n        return len(self.ytrain_imgs)\n    \n    def __getitem__(self, idx):        \n        xtrain = cv.imread(f\"{self.xtrain_path}/{self.xtrain_imgs[idx]}\")\n        xtrain = cv.cvtColor(xtrain, cv.COLOR_BGR2RGB)\n        xtrain = torch.tensor(xtrain).permute(2,0,1) #HWC -> CHW\n        \n        ytrain = cv.imread(f\"{self.ytrain_path}/{self.ytrain_imgs[idx]}\", cv.IMREAD_GRAYSCALE)\n        ytrain = torch.tensor(ytrain)\n        processed_ytrain = self.process_label(ytrain)\n        \n        \n        return xtrain, processed_ytrain ","metadata":{"id":"xT4XzfXFfIlR","execution":{"iopub.status.busy":"2023-06-16T09:00:28.928810Z","iopub.execute_input":"2023-06-16T09:00:28.929899Z","iopub.status.idle":"2023-06-16T09:00:28.940835Z","shell.execute_reply.started":"2023-06-16T09:00:28.929856Z","shell.execute_reply":"2023-06-16T09:00:28.939686Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def visualize_sample(img, label, title):\n    fig, ax = plt.subplots(1, 6, figsize=(20,20))\n    for i, subplot_ax in zip(range(data.num_classes + 1), ax.flatten()):\n        if i == 0: \n            subplot_ax.imshow(img.permute(1,2,0))\n            subplot_ax.set_title(title)\n        else:\n            subplot_ax.imshow(label[i-1], cmap='gray', vmin=0, vmax=1)\n            subplot_ax.set_title(f'Label {i}')\n\ndef visualize_predictions(model, img):\n    inp = (img - 127.5) / 255\n    inp = torch.unsqueeze(inp, dim=0)\n    pred = model(inp)\n    visualize_sample(img, pred.squeeze().detach().numpy(), 'Model Predictions')","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:28.942180Z","iopub.execute_input":"2023-06-16T09:00:28.942989Z","iopub.status.idle":"2023-06-16T09:00:28.951840Z","shell.execute_reply.started":"2023-06-16T09:00:28.942955Z","shell.execute_reply":"2023-06-16T09:00:28.950854Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data = SARData()","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:28.953323Z","iopub.execute_input":"2023-06-16T09:00:28.953989Z","iopub.status.idle":"2023-06-16T09:00:28.976889Z","shell.execute_reply.started":"2023-06-16T09:00:28.953952Z","shell.execute_reply":"2023-06-16T09:00:28.976017Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Network Architecture","metadata":{}},{"cell_type":"code","source":"def double_conv(in_channels, out_channels):\n    conv = nn.Sequential(\n        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1),\n        nn.ReLU(),\n        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1),\n        nn.ReLU()\n    )\n    return conv\n\ndef double_Tconv(in_channels, out_channels):\n    Tconv = nn.Sequential(\n        nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1),\n        nn.ReLU(),\n        nn.ConvTranspose2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1),\n        nn.ReLU()\n    )\n    return Tconv\n\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.conv1 = double_conv(3, 64)\n        self.conv2 = double_conv(64, 128)\n        self.conv3 = double_conv(128, 256)\n        self.conv4 = double_conv(256, 512)\n        self.conv5 = double_conv(512, 1024)\n        self.Tconv5 = double_Tconv(1024, 512)\n        self.Tconv4 = double_Tconv(1024, 256)\n        self.Tconv3 = double_Tconv(512, 128)\n        self.Tconv2 = double_Tconv(256, 64)\n        self.Tconv1 = double_Tconv(32, 5)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.bn4u = nn.BatchNorm2d(512)\n        self.bn3u = nn.BatchNorm2d(256)\n        self.bn2u = nn.BatchNorm2d(128)\n        self.bn1u = nn.BatchNorm2d(64)\n        self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self, x):\n        c1 = self.conv1(x)\n        c1 = self.bn1(c1)\n        x1, i1 = self.max_pool(c1)\n\n        c2 = self.conv2(x1)\n        c2 = self.bn2(c2)\n        x2, i2 = self.max_pool(c2)\n        \n        c3 = self.conv3(x2)\n        c3 = self.bn3(c3)\n        x3, i3 = self.max_pool(c3)\n\n        c4 = self.conv4(x3)\n        c4 = self.bn4(c4)\n        x4, i4 = self.max_pool(c4)\n        \n        i = self.conv5(x4)\n        \n        t4 = self.Tconv5(i)\n        \n        t4 = self.bn4u(t4)\n        z4 = torch.cat((self.max_unpool(t4, i4, output_size=c4.size()), c4), dim=1)\n        t3 = self.Tconv4(z4)\n        \n        t3 = self.bn3u(t3)\n        z3 = torch.cat((self.max_unpool(t3, i3, output_size=c3.size()), c3), dim=1)\n        t2 = self.Tconv3(z3)\n        \n        t2 = self.bn2u(t2)\n        z2 = torch.cat((self.max_unpool(t2, i2, output_size=c2.size()), c2), dim=1)\n        t1 = self.Tconv2(z2)\n        \n        t1 = self.bn1u(t1)\n        z1 = torch.cat((self.max_unpool(t1, i1, output_size=c1.size()), c1), dim=1)\n        z1 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1)(z1)\n        z1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1)(z1)\n        out = self.Tconv1(z1)\n        \n        return self.softmax(out)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:28.978424Z","iopub.execute_input":"2023-06-16T09:00:28.979116Z","iopub.status.idle":"2023-06-16T09:00:28.999503Z","shell.execute_reply.started":"2023-06-16T09:00:28.979082Z","shell.execute_reply":"2023-06-16T09:00:28.998483Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = Network()\ntorchinfo.summary(model, (1,3,256,256))","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:02:13.423235Z","iopub.execute_input":"2023-06-16T09:02:13.423648Z","iopub.status.idle":"2023-06-16T09:02:13.917296Z","shell.execute_reply.started":"2023-06-16T09:02:13.423616Z","shell.execute_reply":"2023-06-16T09:02:13.915849Z"},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n","Cell \u001b[0;32mIn[21], line 79\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m z1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_unpool(t1, i1, output_size\u001b[38;5;241m=\u001b[39mc1\u001b[38;5;241m.\u001b[39msize()), c1), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m z1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(z1)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Network()\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorchinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1]"],"ename":"RuntimeError","evalue":"Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, BatchNorm2d: 1, MaxPool2d: 1, Sequential: 1, Conv2d: 2, ReLU: 2, Conv2d: 2, ReLU: 2, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1, Sequential: 1, ConvTranspose2d: 2, ReLU: 2, ConvTranspose2d: 2, ReLU: 2, BatchNorm2d: 1, MaxUnpool2d: 1]","output_type":"error"}]},{"cell_type":"markdown","source":"### Training and Validation","metadata":{}},{"cell_type":"code","source":"epochs = 50\nbatch_size = 16\nlr = 1e-2\nmomentum=0.9\ntrain, val = torch.utils.data.random_split(data, [0.8, 0.2])","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:29.642591Z","iopub.status.idle":"2023-06-16T09:00:29.643095Z","shell.execute_reply.started":"2023-06-16T09:00:29.642839Z","shell.execute_reply":"2023-06-16T09:00:29.642861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print('Training set pixel distribution')\n# count_1, count_2, count_3, count_4, count_5 = 0,0,0,0,0\n# for _ , label in train:\n#     label = torch.tensor(label)\n#     count_1 += (label[0] == 1).sum()\n#     count_2 += (label[1] == 1).sum()\n#     count_3 += (label[2] == 1).sum()\n#     count_4 += (label[3] == 1).sum()\n#     count_5 += (label[4] == 1).sum()\n# fig = plt.figure()\n# ax = fig.add_axes([0,0,1,1])\n# counts = [count_1, count_2, count_3, count_4, count_5]\n# labels = ['Label1', 'Label2', 'Label3', 'Label4', 'Label5']\n# ax.bar(labels, counts)\n# plt.ylabel('Pixel Counts')\n# plt.show()\n# plt.savefig('train.png')","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:29.645216Z","iopub.status.idle":"2023-06-16T09:00:29.645701Z","shell.execute_reply.started":"2023-06-16T09:00:29.645447Z","shell.execute_reply":"2023-06-16T09:00:29.645468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print('Validation set pixel distribution')\n# count_1, count_2, count_3, count_4, count_5 = 0,0,0,0,0\n# for _ , label in val:\n#     label = torch.tensor(label)\n#     count_1 += (label[0] == 1).sum()\n#     count_2 += (label[1] == 1).sum()\n#     count_3 += (label[2] == 1).sum()\n#     count_4 += (label[3] == 1).sum()\n#     count_5 += (label[4] == 1).sum()\n# fig = plt.figure()\n# ax = fig.add_axes([0,0,1,1])\n# counts = [count_1, count_2, count_3, count_4, count_5]\n# labels = ['Label1', 'Label2', 'Label3', 'Label4', 'Label5']\n# ax.bar(labels, counts)\n# plt.ylabel('Pixel Counts')\n# plt.show()\n# plt.savefig('val.png')","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:29.647669Z","iopub.status.idle":"2023-06-16T09:00:29.648193Z","shell.execute_reply.started":"2023-06-16T09:00:29.647916Z","shell.execute_reply":"2023-06-16T09:00:29.647940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\ntrain, val = torch.utils.data.random_split(data, [0.8, 0.2])\ntrain_size = len(train)\nval_size = len(val)\n\nprint(f'Training with {train_size} images')\nprint(f'Validation with {val_size} images')\n\ntrainloader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\nvalloader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=2)\n\ntotal_train_loss_hist = []\navg_train_loss_hist = []\ntotal_val_loss_hist = []\navg_val_loss_hist = []\n\n\n\nfor epoch in range(epochs):\n    print(f'{\"-\"*15}Epoch {epoch+1}/{epochs}{\"-\"*15}')\n    \n    total_train_loss = 0\n    total_val_loss = 0\n    \n    #Training Loop\n    model.train()\n    for batch, (x,y) in enumerate(trainloader):\n        x, y = x.to(device), y.to(device)\n        x = (x - 127.5) / 255\n        \n        #Forward prop\n        preds = model(x)\n        loss = loss_fn(preds, y)\n        total_train_loss += loss.item()\n        \n        #Backprop\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    #Validation loop\n    model.eval()\n    with torch.no_grad():\n        for batch, (x,y) in enumerate(valloader):\n            #Inference\n            x, y = x.to(device), y.to(device)\n            x = (x - 127.5) / 255\n            preds = model(x)\n            loss = loss_fn(preds, y)\n            total_val_loss += loss.item()\n        \n        \n    \n    avg_train_loss = total_train_loss / train_size\n    avg_val_loss = total_val_loss / val_size\n    \n    print(f\"Avg Training Loss    : {avg_train_loss:<7f}\")\n    print(f\"Avg Validation Loss  : {avg_val_loss:<7f}\")\n    \n    total_train_loss_hist.append(total_train_loss)\n    avg_train_loss_hist.append(avg_train_loss)\n    total_val_loss_hist.append(total_val_loss)\n    avg_val_loss_hist.append(avg_val_loss)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:29.655897Z","iopub.status.idle":"2023-06-16T09:00:29.656335Z","shell.execute_reply.started":"2023-06-16T09:00:29.656104Z","shell.execute_reply":"2023-06-16T09:00:29.656125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.ylabel('Avg Loss per epoch')\nplt.xlabel('Epochs')\nplt.plot(avg_train_loss_hist, label='Avg Training Loss')\nplt.plot(avg_val_loss_hist, label='Avg Validation Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:29.657599Z","iopub.status.idle":"2023-06-16T09:00:29.658545Z","shell.execute_reply.started":"2023-06-16T09:00:29.658243Z","shell.execute_reply":"2023-06-16T09:00:29.658272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Images from the validation set')\nidx = torch.randint(0, val_size, (5,))\nmodel.to('cpu')\nmodel.eval()\nfor i in (idx):\n    img, label = val.__getitem__(i)\n    visualize_sample(img, label, 'Ground Truth')\n    visualize_predictions(model, img)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:29.660253Z","iopub.status.idle":"2023-06-16T09:00:29.660734Z","shell.execute_reply.started":"2023-06-16T09:00:29.660485Z","shell.execute_reply":"2023-06-16T09:00:29.660505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Images from the training set')\nidx = torch.randint(0, train_size, (5,))\nmodel.to('cpu')\nmodel.eval()\nfor i in (idx):\n    img, label = train.__getitem__(i)\n    visualize_sample(img, label, 'Ground Truth')\n    visualize_predictions(model, img)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T09:00:29.662471Z","iopub.status.idle":"2023-06-16T09:00:29.662979Z","shell.execute_reply.started":"2023-06-16T09:00:29.662728Z","shell.execute_reply":"2023-06-16T09:00:29.662751Z"},"trusted":true},"execution_count":null,"outputs":[]}]}