{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport numpy as np\nimport os\nimport math\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\nprint('Running')\n\nxpath='/kaggle/input/sentinel1-sar-images-turkey/Turkey/img_dir'\nypath='/kaggle/input/sentinel1-sar-images-turkey/Turkey/ann_dir'\ntrn_hist_path='/kaggle/output/train_hist'\nval_out_path='/kaggle/output/val_imgs_out'\n\ntry:\n    os.mkdir(trn_hist_path)\n    os.mkdir(val_out_path)\n    print('Created necessary directories')\nexcept:\n    print('Necessary Directories already exist')\n\n#Hyperparameters and Config\n#Training\nbatch_size = 16\nnum_epochs = 50\nlearning_rate=1e-2\nmomentum = 0.9\noptimizer_alg = 'sgd' #'adam' or 'sgd'\n#LR Scheduler\ngamma = 0.9\ndecay_steps = 10\n#Early stopping\nmin_delta=1e-3\npatience=3\n#Outputs\nnum_val_imgs_out=10\n#Network\ndropout_rate = 0.05","metadata":{"_uuid":"c0f69342-e7fd-42a1-9472-a0286390fd95","_cell_guid":"8e98eb11-36dd-4928-b4d4-431c04d55ba9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-06T06:17:26.277364Z","iopub.execute_input":"2023-07-06T06:17:26.278121Z","iopub.status.idle":"2023-07-06T06:17:26.290940Z","shell.execute_reply.started":"2023-07-06T06:17:26.278087Z","shell.execute_reply":"2023-07-06T06:17:26.289708Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Running\nNecessary Directories already exist\n","output_type":"stream"}]},{"cell_type":"code","source":"def visualize_sample(img, label, title, path=''):\n    label = tf.transpose(label, perm=[2,0,1])\n    fig, ax = plt.subplots(1, 6, figsize=(20,20))\n    for i, subplot_ax in zip(range(5 + 1), ax.flatten()):\n        if i == 0: \n            subplot_ax.imshow(img)\n            subplot_ax.set_title(title)\n        else:\n            subplot_ax.imshow(label[i-1], cmap='gray', vmin=0, vmax=1)\n            subplot_ax.set_title(f'Label {i}')\n    if path != '':\n        plt.savefig(f'{path}/{title}.png')\n    \ndef get_iou(preds, label):\n    classwise_iou = []\n    num_classes = 5\n    preds = tf.argmax(preds, axis=-1)\n    label = tf.argmax(label, axis=-1)\n    for c in range(num_classes):\n        tp = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(label, c), tf.equal(preds, c)), dtype=tf.float32))\n        fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.not_equal(label, c), tf.equal(preds, c)), dtype=tf.float32))\n        fn = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(label, c), tf.not_equal(preds, c)), dtype=tf.float32))\n        iou = tp / (tp + fn + fp)\n        if tf.math.is_nan(iou): continue\n        classwise_iou.append(iou)\n    classwise_iou = tf.stack(classwise_iou)\n    miou = tf.reduce_mean(classwise_iou)\n    return miou\n\ndef get_sample_from_val(idx):\n    idx_num = idx % batch_size\n    batch_num = int((idx-idx_num) / batch_size)\n    imgs, labels = val.__getitem__(batch_num)\n    img, label = imgs[idx_num], labels[idx_num]\n    return img, label\n\ndef get_random_val_sample():\n    batch = np.random.randint(0, len(val),(1,))\n    idx = int(np.random.randint(0, batch_size,(1,)))\n    imgs, labels = val.__getitem__(int(batch))\n    return imgs[idx], labels[idx]","metadata":{"_uuid":"6a66b50c-d698-4397-88c5-5ef540225eb5","_cell_guid":"58d461b9-620b-4ef0-a39e-885f71d41643","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-06T06:17:26.293541Z","iopub.execute_input":"2023-07-06T06:17:26.294318Z","iopub.status.idle":"2023-07-06T06:17:26.316681Z","shell.execute_reply.started":"2023-07-06T06:17:26.294276Z","shell.execute_reply":"2023-07-06T06:17:26.315352Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Train(tf.keras.utils.Sequence):\n    def __init__(self, imgs, batch_size=batch_size):\n        self.num_classes = 5\n        self.height, self.width = 256, 256\n        self.xtrain_path = xpath\n        self.ytrain_path = ypath\n        self.imgs = imgs\n        self.batch_size = batch_size\n        \n        self.xtrain = []\n        self.ytrain = []\n        for img_name in self.imgs:\n            xtrain = tf.keras.preprocessing.image.load_img(\n                os.path.join(self.xtrain_path, img_name), target_size=(self.height, self.width)\n            )\n            xtrain = tf.keras.preprocessing.image.img_to_array(xtrain)\n            self.xtrain.append(xtrain)\n\n            ytrain = tf.keras.preprocessing.image.load_img(\n                os.path.join(self.ytrain_path, img_name), target_size=(self.height, self.width),\n                color_mode='grayscale'\n            )\n            ytrain = tf.keras.preprocessing.image.img_to_array(ytrain)\n            ytrain = tf.squeeze(ytrain)\n            ytrain = self.process_label(ytrain)\n            ytrain = tf.transpose(ytrain, perm=[1,2,0])\n            self.ytrain.append(ytrain)\n\n        self.xtrain = tf.stack(self.xtrain)\n        self.ytrain = tf.stack(self.ytrain)\n        self.xtrain = self.xtrain / 255.0\n\n    def __getitem__(self, idx):\n        return self.xtrain[idx * self.batch_size : (idx + 1) * self.batch_size], self.ytrain[idx * self.batch_size : (idx + 1) * self.batch_size]\n\n    def __len__(self):\n        return math.ceil(len(self.imgs) / self.batch_size)\n    \n    def process_label(self, label):\n        r = []\n        for i in range(self.num_classes):\n            mask = tf.cast(tf.equal(label, i+1), dtype=tf.float32)\n            r.append(mask)\n        return tf.stack(r)\n    \nclass Validate(tf.keras.utils.Sequence):\n    def __init__(self, imgs, batch_size=batch_size):\n        self.num_classes = 5\n        self.height, self.width = 256, 256\n        self.xtrain_path = xpath\n        self.ytrain_path = ypath\n        self.imgs = imgs\n        self.batch_size = batch_size\n        \n        self.xtrain = []\n        self.ytrain = []\n        for img_name in self.imgs:\n            xtrain = tf.keras.preprocessing.image.load_img(\n                os.path.join(self.xtrain_path, img_name), target_size=(self.height, self.width)\n            )\n            xtrain = tf.keras.preprocessing.image.img_to_array(xtrain)\n            self.xtrain.append(xtrain)\n\n            ytrain = tf.keras.preprocessing.image.load_img(\n                os.path.join(self.ytrain_path, img_name), target_size=(self.height, self.width),\n                color_mode='grayscale'\n            )\n            ytrain = tf.keras.preprocessing.image.img_to_array(ytrain)\n            ytrain = tf.squeeze(ytrain)\n            ytrain = self.process_label(ytrain)\n            ytrain = tf.transpose(ytrain, perm=[1,2,0])\n            self.ytrain.append(ytrain)\n\n        self.xtrain = tf.stack(self.xtrain)\n        self.ytrain = tf.stack(self.ytrain)\n        self.xtrain = self.xtrain / 255.0\n\n    def __getitem__(self, idx):\n        return self.xtrain[idx * self.batch_size : (idx + 1) * self.batch_size], self.ytrain[idx * self.batch_size : (idx + 1) * self.batch_size]\n\n    def __len__(self):\n        return math.ceil(len(self.imgs) / self.batch_size)\n    \n    def process_label(self, label):\n        r = []\n        for i in range(self.num_classes):\n            mask = tf.cast(tf.equal(label, i+1), dtype=tf.float32)\n            r.append(mask)\n        return tf.stack(r)","metadata":{"_uuid":"72e3bb1b-b9fe-4961-a150-8d69597c8f2e","_cell_guid":"d7ed251a-3aa0-43a8-ade1-880baa28731c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-06T06:17:26.319399Z","iopub.execute_input":"2023-07-06T06:17:26.320367Z","iopub.status.idle":"2023-07-06T06:17:26.349261Z","shell.execute_reply.started":"2023-07-06T06:17:26.320325Z","shell.execute_reply":"2023-07-06T06:17:26.348200Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"imgs = np.array(os.listdir(xpath))\nnp.random.shuffle(imgs)\nimgs = imgs[:1000]\ntrain_imgs, val_imgs = train_test_split(imgs, test_size=0.2)\ntrain = Train(train_imgs)\nval = Validate(val_imgs)","metadata":{"_uuid":"79fd2e0d-6e01-4338-8bbf-f16ac1091ca6","_cell_guid":"3ea163e8-f92d-48dc-b2e7-35196c75ee89","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-06T06:17:26.352191Z","iopub.execute_input":"2023-07-06T06:17:26.352857Z","iopub.status.idle":"2023-07-06T06:20:25.982258Z","shell.execute_reply.started":"2023-07-06T06:17:26.352813Z","shell.execute_reply":"2023-07-06T06:20:25.981104Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def double_conv(in_channels, out_channels):\n    conv = tf.keras.Sequential([\n        layers.Conv2D(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n#         layers.SpatialDropout2D(rate=dropout_rate),\n        layers.Conv2D(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n        layers.BatchNormalization(),\n        layers.Conv2D(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n#         layers.SpatialDropout2D(rate=dropout_rate),\n        layers.Conv2D(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n        layers.BatchNormalization()\n    ])\n    return conv\n\ndef double_Tconv(in_channels, out_channels):\n    Tconv = tf.keras.Sequential([\n        layers.Conv2DTranspose(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n#         layers.SpatialDropout2D(rate=dropout_rate), \n        layers.Conv2DTranspose(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n        layers.BatchNormalization(),\n        layers.Conv2DTranspose(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n#         layers.SpatialDropout2D(rate=dropout_rate),\n        layers.Conv2DTranspose(filters=out_channels, kernel_size=3, strides=1, padding='same'),\n        layers.ReLU(),\n        layers.BatchNormalization()\n    ])\n    return Tconv\n\nclass Network(tf.keras.Model):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.max_pool = layers.MaxPool2D(pool_size=2, strides=2, padding='same')\n        self.upsample = layers.UpSampling2D()\n        self.conv1 = double_conv(3, 64)\n        self.conv2 = double_conv(64, 128)\n        self.conv3 = double_conv(128, 256)\n        self.conv4 = double_conv(256, 512)\n        self.conv5 = double_conv(512, 1024)\n        self.Tconv5 = double_Tconv(1024, 512)\n        self.Tconv4 = double_Tconv(1024, 256)\n        self.Tconv3 = double_Tconv(512, 128)\n        self.Tconv2 = double_Tconv(256, 64)\n        self.Tconv1 = double_Tconv(128, 32)\n        self.bottleneck = layers.Conv2D(filters=5, kernel_size=1, strides=1)\n        self.softmax = layers.Softmax(axis=3)\n        self.dropout = layers.SpatialDropout2D(rate=dropout_rate)\n    \n    def call(self, inputs):\n        \n        #inputs : batch, 256, 256, 3\n        \n        c1 = self.conv1(inputs) #batch, 256, 256, 64\n        x1 = self.max_pool(c1) #batch, 128,128, 64\n\n        c2 = self.conv2(x1) #batch, 128, 128, 128\n        x2 = self.max_pool(c2) #batch, 64, 64, 128\n        \n        c3 = self.conv3(x2) #batch, 64, 64, 256\n        x3 = self.max_pool(c3) #batch, 32, 32, 256\n\n        c4 = self.conv4(x3) #batch, 32, 32, 512\n        x4 = self.max_pool(c4) #batch, 16, 16, 512\n        \n        i = self.conv5(x4) #batch, 16, 16, 1024\n        \n        i = self.dropout(i) #batch, 16, 16, 1024\n        \n        t4 = self.Tconv5(i) #batch, 16, 16, 512\n    \n        z4 = tf.concat([self.upsample(t4), c4], axis=-1) #batch, 32, 32, 1024\n        t3 = self.Tconv4(z4) #batch, 32, 32, 256\n        \n        z3 = tf.concat([self.upsample(t3), c3], axis=-1) #batch, 64, 64, 512\n        t2 = self.Tconv3(z3) #batch, 64, 64, 128\n    \n        z2 = tf.concat([self.upsample(t2), c2], axis=-1) #batch, 32, 32, 256\n        t1 = self.Tconv2(z2) #batch, 128, 128, 64\n        \n        z1 = tf.concat([self.upsample(t1), c1], axis=-1) #batch, 32, 256, 256, 128\n    \n        z0 = self.Tconv1(z1) #batch, 256, 256, 32\n        logits = self.bottleneck(z0) #batch, 256, 256, 5\n        out = self.softmax(logits) #batch, 256, 256, 5\n        return out","metadata":{"_uuid":"5e0ca326-b640-4f82-9c60-8564788ed1eb","_cell_guid":"fb93f629-5482-4790-be98-400d6de93ed6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-06T06:20:25.991657Z","iopub.execute_input":"2023-07-06T06:20:25.995675Z","iopub.status.idle":"2023-07-06T06:20:26.033165Z","shell.execute_reply.started":"2023-07-06T06:20:25.995623Z","shell.execute_reply":"2023-07-06T06:20:26.032108Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = Network()\nmodel.build(input_shape=(batch_size,256,256,3))\nmodel.summary()","metadata":{"_uuid":"831e2385-73f9-4d0e-9a5f-a610469c454d","_cell_guid":"d3503f98-bb9d-4b05-b557-b23444409927","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-06T06:20:26.042949Z","iopub.execute_input":"2023-07-06T06:20:26.043953Z","iopub.status.idle":"2023-07-06T06:20:27.762083Z","shell.execute_reply.started":"2023-07-06T06:20:26.043908Z","shell.execute_reply":"2023-07-06T06:20:27.761157Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"network_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n max_pooling2d_1 (MaxPooling  multiple                 0         \n 2D)                                                             \n                                                                 \n up_sampling2d_1 (UpSampling  multiple                 0         \n 2D)                                                             \n                                                                 \n sequential_10 (Sequential)  (16, 256, 256, 64)        113088    \n                                                                 \n sequential_11 (Sequential)  (16, 128, 128, 128)       517632    \n                                                                 \n sequential_12 (Sequential)  (16, 64, 64, 256)         2067456   \n                                                                 \n sequential_13 (Sequential)  (16, 32, 32, 512)         8263680   \n                                                                 \n sequential_14 (Sequential)  (16, 16, 16, 1024)        33042432  \n                                                                 \n sequential_15 (Sequential)  (16, 16, 16, 512)         11802624  \n                                                                 \n sequential_16 (Sequential)  (16, 32, 32, 256)         4131840   \n                                                                 \n sequential_17 (Sequential)  (16, 64, 64, 128)         1033728   \n                                                                 \n sequential_18 (Sequential)  (16, 128, 128, 64)        258816    \n                                                                 \n sequential_19 (Sequential)  (16, 256, 256, 32)        64896     \n                                                                 \n conv2d_41 (Conv2D)          multiple                  165       \n                                                                 \n softmax_1 (Softmax)         multiple                  0         \n                                                                 \n spatial_dropout2d_10 (Spati  multiple                 0         \n alDropout2D)                                                    \n                                                                 \n=================================================================\nTotal params: 61,296,357\nTrainable params: 61,284,453\nNon-trainable params: 11,904\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate = learning_rate,\n    decay_steps=decay_steps,\n    decay_rate=gamma,\n    staircase=True\n)\ncallbacks=[\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, verbose=1)\n]\n\nif optimizer_alg == 'sgd':\n    optim = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=momentum, nesterov=True)\nelif optimizer_alg == 'adam':\n    optim = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\nmodel.compile(optimizer=optim, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\nhist = model.fit(train, steps_per_epoch=train.__len__(), epochs=num_epochs, validation_data=val, verbose=2)","metadata":{"_uuid":"26f7f1ba-c850-4839-89b7-68993bf500e5","_cell_guid":"f30b99f3-b5f8-40db-8d00-b384aa00d9e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-06T06:20:27.763383Z","iopub.execute_input":"2023-07-06T06:20:27.763818Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"2023-07-06 06:20:35.881109: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape innetwork_1/spatial_dropout2d_10/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"50/50 - 83s - loss: 0.9396 - accuracy: 0.6455 - val_loss: 380.1295 - val_accuracy: 0.2991 - 83s/epoch - 2s/step\nEpoch 2/50\n50/50 - 69s - loss: 0.6811 - accuracy: 0.7361 - val_loss: 11.1253 - val_accuracy: 0.4732 - 69s/epoch - 1s/step\nEpoch 3/50\n50/50 - 70s - loss: 0.6337 - accuracy: 0.7579 - val_loss: 2.1750 - val_accuracy: 0.6696 - 70s/epoch - 1s/step\nEpoch 4/50\n50/50 - 69s - loss: 0.5981 - accuracy: 0.7751 - val_loss: 0.8658 - val_accuracy: 0.7375 - 69s/epoch - 1s/step\nEpoch 5/50\n50/50 - 69s - loss: 0.5702 - accuracy: 0.7895 - val_loss: 0.7122 - val_accuracy: 0.7415 - 69s/epoch - 1s/step\nEpoch 6/50\n50/50 - 69s - loss: 0.5463 - accuracy: 0.7987 - val_loss: 0.6856 - val_accuracy: 0.7154 - 69s/epoch - 1s/step\nEpoch 7/50\n50/50 - 69s - loss: 0.5207 - accuracy: 0.8128 - val_loss: 0.6800 - val_accuracy: 0.7149 - 69s/epoch - 1s/step\nEpoch 8/50\n50/50 - 69s - loss: 0.5018 - accuracy: 0.8221 - val_loss: 0.6688 - val_accuracy: 0.7194 - 69s/epoch - 1s/step\nEpoch 9/50\n50/50 - 69s - loss: 0.4875 - accuracy: 0.8296 - val_loss: 0.6322 - val_accuracy: 0.7406 - 69s/epoch - 1s/step\nEpoch 10/50\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(hist.history['loss'], label='Training Loss')\nplt.plot(hist.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.savefig(f'{trn_hist_path}/loss_curve.png')\nplt.show()\nplt.clf()","metadata":{"_uuid":"7dc9b97e-7a0a-496c-8f3d-c2fab4491388","_cell_guid":"5df4cabb-590c-430b-b13a-9f3cb1167289","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(hist.history['accuracy'], label='Training Accuracy')\nplt.plot(hist.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.savefig(f'{trn_hist_path}/acc_curve.png')","metadata":{"_uuid":"9da165ac-e774-4094-b181-726edce6ec07","_cell_guid":"a400d3df-5721-4b32-a5a0-9ccce237f451","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(val)\niou = get_iou(preds, val.ytrain)\nprint(f'Mean IoU : {iou}')","metadata":{"_uuid":"0a9bfb1d-09ea-4f6f-96ba-2f2065d7418b","_cell_guid":"35056b8e-dfa9-4851-8a6f-255a2190a87c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(num_val_imgs_out):\n    img, label = get_random_val_sample()\n    visualize_sample(img, label, f'Ground Truth {val_imgs[i]}', path=val_out_path)\n    pred = model.predict(tf.expand_dims(img, axis=0))\n    pred = tf.squeeze(pred)\n    pred = tf.argmax(pred, axis=-1) + 1\n    pred = val.process_label(pred)\n    pred = tf.transpose(pred, perm=[1,2,0])\n    visualize_sample(img, tf.squeeze(pred), f'Model Predictions {val_imgs[i]}', path=val_out_path)","metadata":{"_uuid":"f3cffb88-1e04-424d-9424-6e0d52ce0472","_cell_guid":"fb935ff9-2b01-4e67-a59a-931b76efaa52","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}